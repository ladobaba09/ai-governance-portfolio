# AI Acceptable Use Policy

**Policy ID:** LSHA-AI-001  
**Version:** 1.0  
**Effective Date:** January 1, 2026  
**Owner:** Chief Technology Officer  
**Review Cycle:** Annual  
**Next Review:** January 1, 2027

---

## 1. Purpose

This policy establishes acceptable use requirements for artificial intelligence systems at Lone Star Healthcare Analytics (LSHA) to ensure compliance with the Texas Responsible Artificial Intelligence Governance Act (TRAIGA) and alignment with ethical AI principles.

## 2. Scope

This policy applies to:
- All AI systems developed internally by LSHA
- All AI systems procured from third-party vendors
- All employees, contractors, and agents who develop, deploy, or use AI systems
- All AI applications regardless of deployment status (development, testing, production)

## 3. Definitions

**Artificial Intelligence (AI):** A machine-based system that can, for a given set of human-defined objectives, make predictions, recommendations, or decisions influencing real or virtual environments.

**High-Risk AI System:** AI systems making or substantially informing decisions affecting employment, healthcare, financial services, or other consequential domains.

**Deployer:** An entity that uses an AI system, whether developed internally or procured from a vendor.

## 4. Prohibited Uses

The following AI uses are strictly prohibited under TRAIGA and this policy:

### 4.1 Discrimination with Intent
AI systems designed, developed, or deployed with the intent to discriminate against individuals based on protected characteristics including race, color, religion, sex, national origin, age, disability, or genetic information.

### 4.2 Behavioral Manipulation
AI systems that:
- Incite individuals to harm themselves
- Incite individuals to commit acts of violence against others
- Incite individuals to commit criminal acts

### 4.3 Constitutional Violations
AI systems that violate constitutionally protected rights, including but not limited to rights under the First, Fourth, Fifth, and Fourteenth Amendments.

### 4.4 Non-Consensual Imagery
AI systems that generate non-consensual intimate imagery or child sexual abuse material.

### 4.5 Social Scoring (Government Context)
AI systems that assign social scores to individuals based on behavior (applicable if LSHA engages in government contracts).

## 5. Required Documentation

All AI systems must maintain the following documentation:

### 5.1 Business Justification
Written documentation of the legitimate business purpose for deploying the AI system, demonstrating non-discriminatory intent.

### 5.2 Design Intent Statement
Documentation of intended functionality and explicit statement that the system is not designed to discriminate against protected classes.

### 5.3 Testing Records
Records of bias testing, fairness assessments, and validation activities performed on the system.

### 5.4 Change Log
Documentation of all material changes to the AI system, including model updates, feature changes, and threshold adjustments.

## 6. Human Oversight Requirements

### 6.1 Consequential Decisions
AI systems making or informing consequential decisions (employment, healthcare, financial) must include human oversight before adverse actions are taken.

### 6.2 Override Capability
Users must have the ability to override AI recommendations where appropriate.

### 6.3 Escalation Procedures
Clear procedures must exist for escalating AI decisions to human reviewers.

## 7. Transparency Requirements

### 7.1 Internal Disclosure
Employees must be informed when AI systems are used to make decisions affecting their employment.

### 7.2 External Disclosure
Customers and patients must be informed when AI systems substantially influence decisions affecting them, consistent with applicable law (including SB 1188 for healthcare AI).

## 8. Vendor AI Requirements

AI systems procured from third-party vendors must:
- Undergo due diligence assessment before procurement
- Include contractual TRAIGA compliance representations
- Be subject to periodic compliance reviews
- Provide audit access for AI system validation

See AI Vendor Due Diligence Policy (LSHA-AI-002) for detailed requirements.

## 9. Incident Reporting

### 9.1 Reporting Obligation
All employees must report suspected AI-related incidents including:
- Discrimination complaints related to AI decisions
- Unexpected AI system behavior
- Security breaches affecting AI systems
- Regulatory inquiries

### 9.2 Reporting Channel
Reports should be submitted to: ai-governance@lsha.example.com

See AI Incident Response Policy (LSHA-AI-003) for detailed procedures.

## 10. Training Requirements

### 10.1 General Awareness
All employees must complete annual AI ethics and compliance training.

### 10.2 Role-Specific Training
Employees who develop, deploy, or make decisions based on AI systems must complete role-specific training on responsible AI use.

## 11. Compliance and Enforcement

### 11.1 Monitoring
The AI Governance Committee will monitor compliance with this policy through periodic assessments.

### 11.2 Violations
Violations of this policy may result in disciplinary action up to and including termination, as well as potential legal liability.

### 11.3 Safe Harbor
LSHA will utilize the TRAIGA 60-day cure period for good-faith compliance efforts and will maintain NIST AI RMF alignment as an affirmative defense.

## 12. Policy Governance

### 12.1 Owner
The Chief Technology Officer owns this policy with support from the AI Governance Committee.

### 12.2 Review
This policy will be reviewed annually or when material changes occur to applicable law or LSHA's AI portfolio.

### 12.3 Exceptions
Exceptions to this policy require written approval from the CTO and General Counsel.

---

**Approval:**

___________________________ | Date: ___________
Chief Technology Officer

___________________________ | Date: ___________
General Counsel
